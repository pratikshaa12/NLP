{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4l9To30sadWhe0lUTDRtW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikshaa12/NLP/blob/main/NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Word tokenization"
      ],
      "metadata": {
        "id": "dQc5tU-VbFfE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWYvbJG2a7be",
        "outputId": "9ef984a7-44e9-4112-95b4-5f17c7735b17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define your text or import from other source\n",
        "text=' I am learning Natural Language Processing'"
      ],
      "metadata": {
        "id": "uVfjS_N0bA1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizing\n",
        "\n",
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vmcZLwBbA4y",
        "outputId": "3cb0dcb7-7473-44f3-bcec-92db82dea156"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'learning', 'Natural', 'Language', 'Processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Sentence Tokenization"
      ],
      "metadata": {
        "id": "ZCULZHQWbttD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text=\"Our company annual growth rate is 25.50%. Good job Mr.Bajaj\""
      ],
      "metadata": {
        "id": "C3dB4X3vbA7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4SDGcjibA-v",
        "outputId": "394e0451-4aa3-4015-a4e4-d9a8e928c062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Our company annual growth rate is 25.50%.', 'Good job Mr.Bajaj']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regular Expressions"
      ],
      "metadata": {
        "id": "QAp8hzt9cPiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "text=\"Natural language processing 1 is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.\""
      ],
      "metadata": {
        "id": "6hwugfe1cSD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print word by word that contains all small case and starts from small a to z\n",
        "regexp_tokenize(text,\"[a-z]+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7hCwMeqc3Fh",
        "outputId": "df4c3000-6986-45de-bf85-96ec1cac9ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['atural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'is',\n",
              " 'a',\n",
              " 'subfield',\n",
              " 'of',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'and',\n",
              " 'especially',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 't',\n",
              " 'is',\n",
              " 'primarily',\n",
              " 'concerned',\n",
              " 'with',\n",
              " 'providing',\n",
              " 'computers',\n",
              " 'with',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'process',\n",
              " 'data',\n",
              " 'encoded',\n",
              " 'in',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'and',\n",
              " 'is',\n",
              " 'thus',\n",
              " 'closely',\n",
              " 'related',\n",
              " 'to',\n",
              " 'information',\n",
              " 'retrieval',\n",
              " 'knowledge',\n",
              " 'representation',\n",
              " 'and',\n",
              " 'computational',\n",
              " 'linguistics',\n",
              " 'a',\n",
              " 'subfield',\n",
              " 'of',\n",
              " 'linguistics']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extra quote (') get's u word like can't, don't\n",
        "regexp_tokenize(text,\"[a-z']+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f583scdidHCH",
        "outputId": "e0a7db2a-a668-4e4e-ba65-ed23ecaac606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['atural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'is',\n",
              " 'a',\n",
              " 'subfield',\n",
              " 'of',\n",
              " 'computer',\n",
              " 'science',\n",
              " 'and',\n",
              " 'especially',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " 't',\n",
              " 'is',\n",
              " 'primarily',\n",
              " 'concerned',\n",
              " 'with',\n",
              " 'providing',\n",
              " 'computers',\n",
              " 'with',\n",
              " 'the',\n",
              " 'ability',\n",
              " 'to',\n",
              " 'process',\n",
              " 'data',\n",
              " 'encoded',\n",
              " 'in',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'and',\n",
              " 'is',\n",
              " 'thus',\n",
              " 'closely',\n",
              " 'related',\n",
              " 'to',\n",
              " 'information',\n",
              " 'retrieval',\n",
              " 'knowledge',\n",
              " 'representation',\n",
              " 'and',\n",
              " 'computational',\n",
              " 'linguistics',\n",
              " 'a',\n",
              " 'subfield',\n",
              " 'of',\n",
              " 'linguistics']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print word by word that contains all caps and from caps A to Z\n",
        "regexp_tokenize(text,\"[A-Z]+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEI103PsdWGs",
        "outputId": "2b3cded4-3b5f-4ada-858f-6acb1efb834c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N', 'I']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# everythine in on line\n",
        "regexp_tokenize(text,\"[\\a-z']+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6haOkobdWD2",
        "outputId": "2ca23ddd-07a4-44e0-cfc8-a38b883479f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing  is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# anything starts with caret is not equal\n",
        "regexp_tokenize(text,\"[^a-z']+\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHB9PP_OdWBB",
        "outputId": "9093f32e-255a-40ee-fdc4-b47851962d3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['N',\n",
              " ' ',\n",
              " ' ',\n",
              " '  ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " '. I',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ', ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " ', ',\n",
              " ' ',\n",
              " ' ',\n",
              " ' ',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# only numbers\n",
        "regexp_tokenize(text,\"[0-9]+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aue-s8heS-q",
        "outputId": "31763d1b-d2c4-48df-820a-5670882d4b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# without numbers\n",
        "regexp_tokenize(text,\"[^0-9']+\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArLB_GYyedaM",
        "outputId": "995f5754-29c7-4e29-996a-3d38fafe06c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Natural language processing ',\n",
              " ' is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "regexp_tokenize(text,\"[$]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFwrbmffepAA",
        "outputId": "cb54cc96-d338-4d92-9755-c143f4db7fc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stop Words"
      ],
      "metadata": {
        "id": "bow_p4joe_1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYpOTdNDe6KV",
        "outputId": "8f41d565-abb7-4cf8-e51c-b186ba67cfab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words=stopwords.words(\"english\")"
      ],
      "metadata": {
        "id": "UhqnbRE3fHXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5oi3hGofNQt",
        "outputId": "11fb7c4c-4089-40a8-edf2-3daad66dc472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWt9kvNnfWW9",
        "outputId": "f7fbde85-0fb3-49e7-edfd-e0b9c4e98f53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# anothr way\n",
        "stopset=set(nltk.corpus.stopwords.words('english'))"
      ],
      "metadata": {
        "id": "dzlc3l-tfZLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(stopset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nOPeyk_fkB1",
        "outputId": "2903b5d0-da44-42c2-985f-a3d81de8aad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "179"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Stemming"
      ],
      "metadata": {
        "id": "59qcT5O0gUSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer,LancasterStemmer, SnowballStemmer"
      ],
      "metadata": {
        "id": "lkhf_vc1fy9B"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster= LancasterStemmer()\n",
        "porter=PorterStemmer()\n",
        "Snowball=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "Fq6YguX5gfpy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Porter stemmer')\n",
        "\n",
        "print(porter.stem(\"hobby\"))\n",
        "print(porter.stem(\"hobbies\"))\n",
        "print(porter.stem(\"computer\"))\n",
        "print(porter.stem(\"computation\"))\n",
        "print(\"*************************\")\n",
        "\n",
        "print('lancaster stemmer')\n",
        "print(lancaster.stem(\"hobby\"))\n",
        "print(lancaster.stem(\"hobbies\"))\n",
        "print(lancaster.stem(\"computer\"))\n",
        "print(lancaster.stem(\"computation\"))\n",
        "print(\"****************************\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QYrfUY_gwUV",
        "outputId": "3e65470d-f5be-40b7-ca0f-32750c29d633"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter stemmer\n",
            "hobbi\n",
            "hobbi\n",
            "comput\n",
            "comput\n",
            "*************************\n",
            "lancaster stemmer\n",
            "hobby\n",
            "hobby\n",
            "comput\n",
            "comput\n",
            "****************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see with a new sentence\n",
        "sentence= \"I was going to the office on my bike when i saw a car passing by hit the tree.\"\n",
        "token = list(nltk.word_tokenize(sentence))\n",
        "token"
      ],
      "metadata": {
        "id": "CLQh-gnYhoqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "492f0db3-748c-4d6e-fbe8-fbb2af3a9dd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'was',\n",
              " 'going',\n",
              " 'to',\n",
              " 'the',\n",
              " 'office',\n",
              " 'on',\n",
              " 'my',\n",
              " 'bike',\n",
              " 'when',\n",
              " 'i',\n",
              " 'saw',\n",
              " 'a',\n",
              " 'car',\n",
              " 'passing',\n",
              " 'by',\n",
              " 'hit',\n",
              " 'the',\n",
              " 'tree',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bZ1GbAxQkG9T",
        "outputId": "a0df8887-0bed-4e3b-88ef-9437614e138d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I was going to the office on my bike when i saw a car passing by hit the tree.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for stemmer in (Snowball,lancaster, porter):\n",
        "  stemm = [stemmer.stem(t) for t in token]\n",
        "  print(\" \".join(stemm))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwBaXx-4kJTz",
        "outputId": "8b4fd31c-aeb9-45b4-c2d5-d97cb9a6891f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i was go to the offic on my bike when i saw a car pass by hit the tree .\n",
            "i was going to the off on my bik when i saw a car pass by hit the tre .\n",
            "i wa go to the offic on my bike when i saw a car pass by hit the tree .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Fa5jSqUkJRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### lancaster algorithm is faster than porter but it is more complex. porter stemmer is the oldest algorithm present present and was the most popular to use.\n",
        "Snowball stemmer also known as porter2. is the updated version of the porter stemmer and is currently the most popular.\n",
        "snowball stemmer is available for multiple languages as well."
      ],
      "metadata": {
        "id": "FeSHUWOfkybl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one more simple example of porter\n",
        "print(porter.stem(\"running\"))\n",
        "print(porter.stem(\"runs\"))\n",
        "print(porter.stem(\"ran\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJbE-PRwkJOG",
        "outputId": "a1aa0417-8e76-4067-9977-80393caa4840"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "run\n",
            "ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Here we can see the lemma has changed for the words with same base.\n",
        "This is becoz we haven't given any context to the lemmatizer.\n",
        "Generally it is given by passing  the POS tags for the words in a sentence."
      ],
      "metadata": {
        "id": "oyO6KxBErTTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(porter.stem(\"running\", pos='v'))\n",
        "print(porter.stem(\"runs\", pos='v'))\n",
        "print(porter.stem(\"ran\", pos='v'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "d6AIEasJkJLi",
        "outputId": "fa35580a-67d9-4fc1-8e88-aae261aba9a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "PorterStemmer.stem() got an unexpected keyword argument 'pos'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-fca7f67a98f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"running\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"runs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ran\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'v'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: PorterStemmer.stem() got an unexpected keyword argument 'pos'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatization"
      ],
      "metadata": {
        "id": "5VPeRjtqsgkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9yysoa9shO_",
        "outputId": "1be5f4da-b431-4fd6-d692-7fd5412bdcdc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize(\"running\"))\n",
        "print(lemma.lemmatize(\"runs\"))\n",
        "print(lemma.lemmatize(\"ran\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMidufFskJIx",
        "outputId": "9162bc0e-d5d4-477c-c0a8-29df4bef89f3"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\n",
            "run\n",
            "ran\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can see the lemma has changed for the words with same base.\n",
        "This is becoz we haven't given any context to the lemmatizer. Generally it is given by passing the POS tags for the words in a sentence.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZS0W-sB0tBhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemma.lemmatize(\"running\",pos='v'))\n",
        "print(lemma.lemmatize(\"runs\",pos='v'))\n",
        "print(lemma.lemmatize(\"ran\",pos='v'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPJFrdF4s58F",
        "outputId": "1d87d1e0-3c83-4b3a-8be9-29fb5457fc08"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n",
            "run\n",
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one more example using both stemming and lemma\n",
        "text=\"Bring king going Anything sing ring nothing thing\"\n",
        "\n",
        "# stemmimg\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "tokenization=nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "\n",
        "  print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaZDGgZ5ugZ0",
        "outputId": "ce873b2e-e4be-48ad-f773-c482a6419b67"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemming for Bring is bring\n",
            "Stemming for king is king\n",
            "Stemming for going is go\n",
            "Stemming for Anything is anyth\n",
            "Stemming for sing is sing\n",
            "Stemming for ring is ring\n",
            "Stemming for nothing is noth\n",
            "Stemming for thing is thing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# lemma\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer= WordNetLemmatizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "tokenization=nltk.word_tokenize(text)\n",
        "for w in tokenization:\n",
        "\n",
        "  print(\"Lemma for {} is {}\".format(w,wordnet_lemmatizer.lemmatize(w)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_38TAaFvjhD",
        "outputId": "be42353e-5710-46d0-a32c-043411061f89"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemma for Bring is Bring\n",
            "Lemma for king is king\n",
            "Lemma for going is going\n",
            "Lemma for Anything is Anything\n",
            "Lemma for sing is sing\n",
            "Lemma for ring is ring\n",
            "Lemma for nothing is nothing\n",
            "Lemma for thing is thing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Lemmatization takes more time to give the response."
      ],
      "metadata": {
        "id": "oZFOBgvHwpH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Wordnet"
      ],
      "metadata": {
        "id": "vlwhUMBkwxk2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### wordnet is an NLTK corpus reader, a lexical database for english. it can be used to find the meaning of words, synonym, or antonym."
      ],
      "metadata": {
        "id": "tgxSNcLjw5lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "II0LqnnVwcuu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets find sysnonms and antonym using python code\n",
        "from nltk.corpus import wordnet\n",
        "synonyms= []\n",
        "antonyms= []\n",
        "\n",
        "for syn in wordnet.synsets(\"active\"):\n",
        "  for l in syn.lemmas():\n",
        "    synonyms.append(l.name())\n",
        "    if l.antonyms():\n",
        "      antonyms.append(l.antonyms()[0].name())\n",
        "print('synonyms  =>', set(synonyms))\n",
        "print('antonyms =>', set(antonyms))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD-6o8UExPzz",
        "outputId": "6b4ea9f8-5b52-4e05-c0a1-bfb63f39bae8"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "synonyms  => {'combat-ready', 'active', 'fighting', 'dynamic', 'active_voice', 'participating', 'active_agent', 'alive'}\n",
            "antonyms => {'passive_voice', 'quiet', 'passive', 'inactive', 'dormant', 'stative', 'extinct'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cypOAEIzyS4W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}